<!DOCTYPE html>
<html>
<head>
    <title>Why DRL is hard?</title>
</head>
<body>
    <h1></h1>
<article><ul>
<li><a href="https://towardsdatascience.com/a-short-introduction-to-go-explore-c61c2ef201f0">How to Exploration</a></li>
<li><a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Why RL Hard</a></li>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/bdgxin/d_any_papers_that_criticize_deep_reinforcement/">DRL Sucks</a></li>
</ul>
<h2><a href="https://ai.stackexchange.com/questions/7763/how-to-define-states-in-reinforcement-learning">State Representation</a></h2>
<ul>
<li>state vector ( obey Markov Property)</li>
<li>observation →knowledge→ state</li>
<li><a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">Partially observable Markov decision process</a></li>
<li>&quot;learning or classification algorithms to &quot;learn&quot; those states&quot;
<ul>
<li>A simple linear regression</li>
<li>A more complex non-linear function approximator, such as a multi-layer neural network.</li>
</ul>
</li>
</ul>
<p>The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation. The DNN then processed the images into higher-level features that could be used to make predictions about state values.</p>
</article>
</body>
</html>