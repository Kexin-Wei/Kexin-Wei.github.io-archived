<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Basic Recap #  Reinforcement learning bases on   \(V(s),Q(s,a),\pi(a|s),R,G\)  :
   \(V(s)\)  : state value, often used in model-based method;
   \(Q(s,a)\)  : state-action value, often used in model-free method;
 why state-action:  \(s\rightarrow a\)  is defined partly in  \(\pi(a|s)\)  , and  \(V(s,a),\pi(a|s)\)  are all parameters inside agent, consequently,  \(Q(s,a)\)  is a combination of  \(V(s)\)  and  \(\pi(a|s)\)  .">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="Monte Carlo vs TD vs Q-learning" />
<meta property="og:description" content="Basic Recap #  Reinforcement learning bases on   \(V(s),Q(s,a),\pi(a|s),R,G\)  :
   \(V(s)\)  : state value, often used in model-based method;
   \(Q(s,a)\)  : state-action value, often used in model-free method;
 why state-action:  \(s\rightarrow a\)  is defined partly in  \(\pi(a|s)\)  , and  \(V(s,a),\pi(a|s)\)  are all parameters inside agent, consequently,  \(Q(s,a)\)  is a combination of  \(V(s)\)  and  \(\pi(a|s)\)  ." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://vacationwkx.github.io/docs/ai/rl-comparsion/" /><meta property="article:section" content="docs" />
<meta property="article:published_time" content="2020-11-28T21:47:38+08:00" />
<meta property="article:modified_time" content="2021-10-20T00:21:01+08:00" />

<title>Monte Carlo vs TD vs Q-learning | ç„¡æ¥µ</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.95d69eb6bad8b9707ff2b5d8d9e31ce70a1b84f2ed7ffaf665ffcf00aa7993bd.css" integrity="sha256-ldaetrrYuXB/8rXY2eMc5wobhPLtf/r2Zf/PAKp5k70=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.7bbcf3387737599ae496d8e131ef5cde32bd589e961f31a2784d5b933c790ec5.js" integrity="sha256-e7zzOHc3WZrkltjhMe9c3jK9WJ6WHzGieE1bkzx5DsU=" crossorigin="anonymous"></script>

  <script defer src="/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js" integrity="sha256-b2&#43;Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC&#43;NdcPIvZhzk=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><img src="/favicon.ico" alt="Logo" /><span>ç„¡æ¥µ</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>



  



  
    
  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex justify-between">
      <a role="button" class="flex align-center">
        <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
        English
      </a>
    </label>

    <ul>
      
      <li>
        <a href="https://vacationwkx.github.io/ru/">
          Deutsch
        </a>
      </li>
      
      <li>
        <a href="https://vacationwkx.github.io/zh/">
          Chinese
        </a>
      </li>
      
    </ul>
  </li>
</ul>






  
<ul>
  
  <li>
    <a href="/posts/" >
        Working on ...
      </a>
  </li>
  
</ul>







  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8bb89e37bc8ac94ed7933c2c15518f8c" class="toggle" checked />
    <label for="section-8bb89e37bc8ac94ed7933c2c15518f8c" class="flex justify-between">
      <a role="button" class="">1. AI</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/ai/rl-notes/" class="">Reinforcement Learning Notes</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/ai/nn-notes/" class="">NNN Learning Notes</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/ai/rl-epsilon-policy/" class="">epsilon-Greedy vs epsilon-Soft</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/ai/rl-update-equation/" class="">Value Update Comparison among Basic RL</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/ai/rl-comparsion/" class=" active">Monte Carlo vs TD vs Q-learning</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-010cc3a6dda69f98986133a7094047b7" class="toggle"  />
    <label for="section-010cc3a6dda69f98986133a7094047b7" class="flex justify-between">
      <a role="button" class="">2. Books I Read</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/books/simple-europe-history/" class="">The Shortest History of Europe</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/books/The-Crowd/" class="">The Crowd - A Study of the Popular Mind</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/books/antifragile/" class="">Antigragile</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-d808f8574b076f34757efa25cfa94f05" class="toggle"  />
    <label for="section-d808f8574b076f34757efa25cfa94f05" class="flex justify-between">
      <a role="button" class="">3. Learning Plan</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/learn/front-end/" class="">Front End</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/learn/AI/" class="">AI</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-2a14d3e9b56ded9a228b0add698b5e2b" class="toggle"  />
    <label for="section-2a14d3e9b56ded9a228b0add698b5e2b" class="flex justify-between">
      <a role="button" class="">4. Diary</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/diary_thoughts/Thoughts-In-2019/" class="">Thoughts in 2019</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/diary_thoughts/tour2/" class="">2017-12-01(2)-Not a serious Diary</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/diary_thoughts/tour/" class="">2017-12-01(1)-Not a serious Diary</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/diary_thoughts/CSF/" class="">Der Tod eines Mannen - ChenShifeng</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/diary_thoughts/saenger/" class="">Der Saenger</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-9a5dc60c61925a1b6716e156a8b79c04" class="toggle"  />
    <label for="section-9a5dc60c61925a1b6716e156a8b79c04" class="flex justify-between">
      <a role="button" class="">Archived</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/achv/ssh-jupyter/" class="">Use Jupiter Notebook by ssh</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/achv/rustylake/" class="">Rusty lake - Game Timeline &amp; Secrets</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/achv/new-on-hugo/" class="">New on HugoðŸ¤º</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/achv/Learn-Jekyll/" class="">Note Learn Jekyll</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://vacationwkx.github.io/docs/achv/GRE/" class="">Personal GRE Summary</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>











  
<ul>
  
  <li>
    <a href="https://github.com/Vacationwkx" target="_blank" rel="noopener">
        Github
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Monte Carlo vs TD vs Q-learning</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#basic-recap">Basic Recap</a>
      <ul>
        <li><a href="#a-little-more-from-basic">A little more from Basic</a></li>
        <li><a href="#the-famous-bellman-equation">The FAMOUS Bellman Equation</a></li>
        <li><a href="#choose-path-based-on-bellman-equation">Choose Path based on Bellman Equation</a></li>
        <li><a href="#v-value-vs-q-value">V value vs Q value</a></li>
      </ul>
    </li>
    <li><a href="#update-methods-clarification">Update Methods Clarification</a>
      <ul>
        <li><a href="#monte-carlo">Monte Carlo</a></li>
        <li><a href="#a-constant-monte-carlo">a-constant Monte Carlo</a></li>
        <li><a href="#temporal-difference">Temporal Difference</a></li>
        <li><a href="#q-learning">Q-learning</a></li>
      </ul>
    </li>
    <li><a href="#other-thinking">Other Thinking</a>
      <ul>
        <li><a href="#arbitrary-initial-q-or-v">Arbitrary Initial Q or V</a></li>
        <li><a href="#where-goes-the-transition-function">Where goes the transition function?</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="basic-recap">
  Basic Recap
  <a class="anchor" href="#basic-recap">#</a>
</h1>
<p>Reinforcement learning bases on 
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(V(s),Q(s,a),\pi(a|s),R,G\)
</span>
:</p>
<ul>
<li>
<p><span>
  \(V(s)\)
</span>
 : state value, often used  in model-based method;</p>
</li>
<li>
<p><span>
  \(Q(s,a)\)
</span>
 : state-action value, often used in model-free method;</p>
<ul>
<li>why state-action: <span>
  \(s\rightarrow a\)
</span>
 is defined partly in <span>
  \(\pi(a|s)\)
</span>
, and <span>
  \(V(s,a),\pi(a|s)\)
</span>
 are all parameters inside agent, consequently,  <span>
  \(Q(s,a)\)
</span>
 is a combination of <span>
  \(V(s)\)
</span>
 and <span>
  \(\pi(a|s)\)
</span>
.</li>
</ul>
</li>
<li>
<p><span>
  \(\pi(a|s)\)
</span>
 : the policy of a agent, chose a <span>
  \(a\)
</span>
 (action) at a <span>
  \( s \)
</span>
 state;</p>
</li>
<li>
<p><span>
  \(R\)
</span>
 : reward, got from each step</p>
</li>
<li>
<p><span>
  \(G\)
</span>
 : a time-scale reward recording, or a estimate of value for current state.
<span>
  \[  G_t=R_T&#43;\gamma R_{T-1}&#43;\gamma^2R_{T-2}&#43;...=\sum\limits_{t&#43;1}^{T}\gamma^{T-i} R
  \]
</span>
</p>
<ul>
<li><span>
  \(T\)
</span>
 : Terminal time</li>
<li><span>
  \(\gamma\)
</span>
 : a self-defined parameter to look how much further into future &ndash; long future reward would not affect that much,but instant does.</li>
<li>From the equation, the <span>
  \(G\)
</span>
 is influenced by <span>
  \(R\)
</span>
 and <span>
  \(\gamma\)
</span>
, but for a well-behaved future-telling agent <span>
  \(\gamma\)
</span>
 is usually set to 1or 0.9, which indicates, for a self-made envornment, <span>
  \(R\)
</span>
 should be set properly to obtain a wanted training result.</li>
</ul>
</li>
</ul>
<h2 id="a-little-more-from-basic">
  A little more from Basic
  <a class="anchor" href="#a-little-more-from-basic">#</a>
</h2>
<p><strong>Example</strong> : a hero in game, collects he always coins(reward) along a path in a 2d grid map to gain experience</p>
<p>
  <img src="https://miro.medium.com/freeze/max/588/1*Lq_shZnfjjiFEBmBOHk_qA.gif" alt="a hero in a 2D map" />
<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<ul>
<li>
<p>Real Existing Items:</p>
<p>Once the hero has the real items, it can absolutely get the max reward from environment.</p>
<ul>
<li><span>
  \(G\)
</span>
 represents how many future values the position has, (even <span>
  \(\gamma\)
</span>
 is also self-defined, but in my view, <span>
  \(\gamma\)
</span>
 doesn&rsquo;t affect that much.)</li>
<li>and <span>
  \(R\)
</span>
 is what the hero gets from each step in the environment.</li>
</ul>
</li>
<li>
<p>Esitimate:</p>
<p>Estimate is what the hero guess about the <span>
  \(G\)
</span>
, which is <span>
  \(E(G)\)
</span>
. But obviously, in an environment, <span>
  \(G\)
</span>
 is related to state and time, when the hero is exploring with a policy. Then <span>
  \(E(G)\)
</span>
 should be <span>
  \(E_{\pi}(G_t|S_t=s)\)
</span>
, that&rsquo;s what we get from training.</p>
<ul>
<li><span>
  \(v_{\pi}(s)\)
</span>
 - value function</li>
<li><span>
  \(q_{\pi}(s,a)\)
</span>
 - action-value function</li>
</ul>
<p>These 2 are generally the same with <span>
  \(V(s),Q(s,a)\)
</span>
, since basically the policy always exist for most of the agent. The only difference is now they are estimate for <span>
  \(G\)
</span>
 with policy <span>
  \(\pi\)
</span>
.</p>
</li>
</ul>
<h2 id="the-famous-bellman-equation">
  The FAMOUS Bellman Equation
  <a class="anchor" href="#the-famous-bellman-equation">#</a>
</h2>
<p>The Bellman equation is basiccally connecting the <span>
  \(v_{\pi}(s)\)
</span>
 and <span>
  \(v_{\pi}(s&#39;)\)
</span>
, or <span>
  \(q_{\pi}(s,a)\)
</span>
 and <span>
  \(q_{\pi}(s&#39;,a&#39;)\)
</span>
,</p>
<p>
  <img src="/rl/vs.png" alt="Backup Diagramm" /><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>
<span>
  \[\begin{aligned}
v_{\pi}(s)&amp;=E_{\pi}[G_t|S_t=s]\\
&amp;=E_{\pi}[R_{t&#43;1}&#43;\gamma G_{t&#43;1}|S_t=s]\\
&amp;=?E_{\pi}[R_{t&#43;1}&#43;\gamma G_{t&#43;1}|S_{t&#43;1}=s&#39;]\\
&amp;=\sum_{a\in A}\pi(a|s)\sum_{s&#39;\in S}p(s&#39;,r|s,a) E_{\pi}[R_{t&#43;1}&#43;\gamma G_{t&#43;1}|S_{t&#43;1}=s&#39;]\\
&amp;=\sum_{a\in A}\pi(a|s)\sum_{s&#39;\in S}p(s&#39;,r|s,a) [r&#43;\gamma E_{\pi}[G_{t&#43;1}|S_{t&#43;1}=s&#39;]\\
&amp;=\sum_{a\in A}\pi(a|s)\sum_{s&#39;\in S}p(s&#39;,r|s,a) [r&#43;\gamma v_{\pi}(s&#39;)]\\
\end{aligned}\]
</span>
</p>
<p>Similarily explained above, <span>
  \(E(G)\)
</span>
 will become <span>
  \(E_{\pi}(G_t|S_t=s,A_t=a)\)
</span>
 for <span>
  \(q_{\pi}(s,a)\)
</span>
, then the Bellman equation changes to:</p>
<span>
  \[\begin{aligned}
q_{\pi}(s,a)&amp;=E_{\pi}[G_t|S_t=s,A_t=a]\\
&amp;=?E_{\pi}[R_{t&#43;1}&#43;\gamma G_{t&#43;1}|S_{t&#43;1}=s&#39;,A_{t&#43;1}=a&#39;]\\
&amp;=\sum_{s&#39;\in S}p(s&#39;,r|s,a)E_{\pi}[R_{t&#43;1}&#43;\gamma G_{t&#43;1}|S_{t&#43;1}=s&#39;]\\
&amp;=\sum_{s&#39;\in S}p(s&#39;,r|s,a)\sum_{a&#39;\in A}\pi(a&#39;|s&#39;)E_{\pi}[R_{t&#43;1}&#43;\gamma G_{t&#43;1}|S_{t&#43;1}=s&#39;,A_{t&#43;1}=a&#39;]\\
&amp;=\sum_{s&#39;\in S}p(s&#39;,r|s,a)\sum_{a&#39;\in A}\pi(a&#39;|s&#39;)[r&#43;\gamma E_{\pi}[G_{t&#43;1}|S_{t&#43;1}=s&#39;,A_{t&#43;1}=a&#39;]]\\
&amp;=\sum_{s&#39;\in S}p(s&#39;,r|s,a)\sum_{a&#39;\in A}\pi(a&#39;|s&#39;)[r&#43;\gamma q_{\pi}(s&#39;,a&#39;)]\\
\end{aligned}\]
</span>

<h2 id="choose-path-based-on-bellman-equation">
  Choose Path based on Bellman Equation
  <a class="anchor" href="#choose-path-based-on-bellman-equation">#</a>
</h2>
<p>When the hero stand at <span>
  \(s\)
</span>
 state seeing all <span>
  \(v_{\pi}(s&#39;)\)
</span>
 , but only one step will be chosen in reality, which means <span>
  \(\pi(a|s)=1\)
</span>
 for this action <span>
  \(a\)
</span>
. This decision will let the <span>
  \(v_{\pi}(s)\)
</span>
 biggest, and the policy will be updated and <span>
  \(v_*(s)\)
</span>
 is defined as:
<span>
  \[\begin{aligned}
v_*(s)&amp;=\max_a \pi(a|s)\sum_{s&#39;\in S}p(s&#39;,r|s,a)[r&#43;\gamma v_{\pi}(s&#39;)]\\
&amp;=\sum_{s&#39;\in S}p(s&#39;,r|s,a_{\max})[r&#43;\gamma v_{\pi}(s&#39;)]\\
&amp;=q_{\pi}(s,a_{\max})
\end{aligned}\]
</span>
</p>
<p><span>
  \(p(s&#39;,r|s,a)\)
</span>
 is of course not controlled by the hero, thus, policy has the only option in next step &ndash; at <span>
  \(s&#39;\)
</span>
 choose <span>
  \(a&#39;_{\max}\)
</span>
 , where <span>
  \(q(s&#39;,a&#39;)\)
</span>
 is max for all <span>
  \(a&#39; \in A\)
</span>
. Use the same logic,</p>
<span>
  \[\begin{aligned}
q_*(s,a)&amp;=\sum_{s&#39;\in S}p(s&#39;,r|s,a)[r&#43;\gamma q(s&#39;,a&#39;_{\max})]\\
\end{aligned}\]
</span>

<h2 id="v-value-vs-q-value">
  V value vs Q value
  <a class="anchor" href="#v-value-vs-q-value">#</a>
</h2>
<p><span>
  \(q_{\pi}(s,a)\)
</span>
 seems have chosen the <span>
  \(a\)
</span>
 without policy. But thinking deeply, policy <span>
  \(\pi(a|s)\)
</span>
 controls the choice when finally the hero acts in the environment. The <span>
  \(\pi\)
</span>
 for <span>
  \(v(s)\)
</span>
 and <span>
  \(q(s,a)\)
</span>
 just dedicates the policy is updated according <span>
  \(v(s)\)
</span>
 and <span>
  \(q(s,a)\)
</span>
.</p>
<p>No matter which is used in policy update, what really matters is the next state <span>
  \(s&#39;\)
</span>
, the <span>
  \(v(s&#39;)\)
</span>
 or the <span>
  \(\sum\limits_{s&#39;\in S} p(s&#39;,r|s,a)[r&#43;\gamma v(s&#39;)] \)
</span>
, since again the <span>
  \(p(s&#39;,r|s,a)\)
</span>
 is not controllable.</p>
<p>Once the next step is determinated, <span>
  \(a\)
</span>
 at this state <span>
  \(s\)
</span>
 is also confirmed. <span>
  \(q(s,a)\)
</span>
 just more connects to the next state.</p>
<p><span>
  \(v(s)\)
</span>
 choses the path by comparsion between multiple <span>
  \(v(s&#39;)\)
</span>
, but <span>
  \(q(s,a)\)
</span>
 indicates the path by comparsion between its company <span>
  \(q(s,a_1), q(s,a_2), q(s,a_3)...\)
</span>
.</p>
<h1 id="update-methods-clarification">
  Update Methods Clarification
  <a class="anchor" href="#update-methods-clarification">#</a>
</h1>
<p>Monte Carlo, Temporal-Difference and Q-Learning are all model-free methods, which means the probability departing from states to states is unknown. The above optimal policy is used in Dynamic Programming, since the <span>
  \(p(s&#39;,r|s,a)\)
</span>
 is known. That&rsquo;s also the reason why use DP in model-based environment. For model-free environment, the value is estimated by exploring and update. MC, TD or Q-learning just differ at these 2 processes.</p>
<h2 id="monte-carlo">
  Monte Carlo
  <a class="anchor" href="#monte-carlo">#</a>
</h2>
<p>The basic idea of Monte Carlo is to estimate value by :
<span>
  \[V(s)=\frac{G}{N}\]
</span>
</p>
<p>in the step update form:
<span>
  \[V(s)\leftarrow V(s)&#43;\frac{R-V(s)}{N}\]
</span>
</p>
<p>with starting from <span>
  \(N=1\)
</span>
, in Monte Carlo <span>
  \(V(s)=G\)
</span>
.</p>
<p>With this setting, Monte Carlo performs the best with full exploring, also means <span>
  \(\epsilon=1\)
</span>
 for on policy MC control with <span>
  \(\epsilon\)
</span>
-soft algorithm, and must run enough steps, which is absolutely slow!!!</p>
<p>Using this idea, of course in most environments, exploring start with argmax at policy update will fail.</p>
<p>Nevertheless, the <span>
  \(G\)
</span>
 is driven from trajecotry <span>
  \(\left[s_0,a_0,r_1,s_1,a_1,r_2,...,s_{T-1},a_{T-1},r_T\right]\)
</span>
 updated by <span>
  \(G_{s_t}=R_t&#43;\gamma G_{s_{t-1}}\)
</span>
, where the Terminal <span>
  \(G_{s_T}=0\)
</span>
. No terminal then no value update and policy update. However, a random walk can&rsquo;t garante reaching the terminal.</p>
<h2 id="a-constant-monte-carlo">
  a-constant Monte Carlo
  <a class="anchor" href="#a-constant-monte-carlo">#</a>
</h2>
<p>The <span>
  \(\alpha\)
</span>
 - constant Monte Carlo updates it by:
<span>
  \[\begin{aligned}
V(s)&amp;=V(s)&#43;\alpha \left[G-V(s)\right]\\
&amp;=V(s)&#43;\frac{G-V(s)}{\frac{1}{\alpha}}\\
&amp;=V(s)(1-\alpha)&#43;\alpha G
\end{aligned}\]
</span>
</p>
<p>In <span>
  \(\alpha\)
</span>
 - constant will always consider part of the original <span>
  \(V(s)\)
</span>
: <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<span>
  \[\begin{aligned}
V_{ep&#43;1}(s)&amp;=\left[V_{ep-1}(s)(1-\alpha)&#43;\alpha G_{ep-1}\right](1-\alpha)&#43;\alpha G_{ep}\\
&amp;=V_{ep-1}(1-\alpha)^2&#43;\alpha(1-\alpha)G_{ep-1}&#43;\alpha G_{ep}\\
&amp;=V_1(1-\alpha)^{ep}&#43;\sum_1^{ep}\alpha(1-\alpha)^iG_i\\
\end{aligned}\]
</span>

<p>for <span>
  \(\alpha &lt;1\)
</span>
, when <span>
  \(t\rightarrow \infty\)
</span>
,  <span>
  \(V_{\infty}\)
</span>
 has more value depending on <span>
  \(G\)
</span>
, and specially recent <span>
  \(G\)
</span>
.</p>
<p>What&rsquo;s more, when updating the value, the value <span>
  \(V(s)\)
</span>
 is moving towards to the actual value, no matter is updated by Monte Carlo average method or TD or Q-learning, so partly we can trust the new <span>
  \(V(s)\)
</span>
.</p>
<h2 id="temporal-difference">
  Temporal Difference
  <a class="anchor" href="#temporal-difference">#</a>
</h2>
<p>TD is a bootstrapping method, which is quiet determined by the old value.
<span>
  \[V_{ep&#43;1}(s)=V_{ep}(s)&#43;\alpha[R&#43;\gamma V_{ep}(s&#39;)-V_{ep}(s)]\]
</span>
</p>
<p>Comparing with the <span>
  \(\alpha\)
</span>
 - constant Monte Carlo <span>
  \(V_{ep&#43;1}(s)=V_{ep}(s)&#43;\alpha [R_{ep}&#43;\gamma G_{ep-1}-V_{ep}(s)]\)
</span>
, <span>
  \(\alpha\)
</span>
 is the stepsize and also determines the update quantity of the <span>
  \(V(s)\)
</span>
. Once <span>
  \(V(s&#39;)\)
</span>
 is estimated close to the real value, <span>
  \(V(s)\)
</span>
 is updated by one step closer to the real <span>
  \(V(s)\)
</span>
. Digging to the end, the terminal <span>
  \(V(s_T)=0\)
</span>
, and the <span>
  \(V(s_{T-1})\)
</span>
 s are all updated exactlly by one step close to the real value, unlike the Monte Carlo, always needing a trajectory to end to update the value.</p>
<p>For TD, update is not deserved with end to terminal. The first run to terminal is only updated valuable on the <span>
  \(V(s_{T-1})\)
</span>
, and next run is <span>
  \(V(s_{T-2})\)
</span>
, and so on&hellip;</p>
<p>On one side, the <span>
  \(V(s)\)
</span>
 is updated truely along the way to terminal, with this chosen path, the value is updated more fast, since the agent prefers to go this path under <span>
  \(\epsilon\)
</span>
 - greedy policy; On the other side, with randomly exploring, the agent searchs for a better way to terminal. Once found the new path will be compared with the old one, the <span>
  \(V(s)\)
</span>
 will determine the optimal path.</p>
<p>If we use <span>
  \(Q(s,a)\)
</span>
 in TD, then the algorithm is called the famous <strong>sarsa</strong>.
<span>
  \[Q_{ep&#43;1}(s,a)=Q_{ep}(s,a)&#43;\alpha\left[R&#43;\gamma Q_{ep}(s&#39;,a&#39;)-Q_{ep}(s,a)\right]\]
</span>

Similarly, the <span>
  \(Q(s,a)\)
</span>
 is updated from the <span>
  \(Q(s_T,a_T)\)
</span>
 once reaches the terminal.</p>
<h2 id="q-learning">
  Q-learning
  <a class="anchor" href="#q-learning">#</a>
</h2>
<p>While the agent is still randomly walking in the environment without arriving at the terminal, then the updated value is equavalent to random initialized <span>
  \(Q(s,a)\)
</span>
. The meaningful value is like TD starting from <span>
  \(Q(s_T,a_T)\)
</span>
, the difference locates at that, because of the continous exploring, we can safely choose the best way with fast speed. This indicates we can determine the best step from state <span>
  \(s\)
</span>
 by looking into the <span>
  \(Q(s&#39;,a&#39;)\)
</span>
s and gives the <span>
  \(s-1\)
</span>
 a symbol (<span>
  \(Q(s,a)\)
</span>
) that <span>
  \(s\)
</span>
 is the best within his company:
<span>
  \[Q_{ep&#43;1}(s,a)=Q_{ep}(s,a)&#43;\alpha \left[R&#43;\gamma Q_{ep}(s&#39;,a&#39;_{\max})-Q_{ep}(s,a)\right]\]
</span>

Gradually the from the starting state, the agent find the fast by seeing the biggest <span>
  \(Q(s,a)\)
</span>
 at each state.</p>
<h1 id="other-thinking">
  Other Thinking
  <a class="anchor" href="#other-thinking">#</a>
</h1>
<h2 id="arbitrary-initial-q-or-v">
  Arbitrary Initial Q or V
  <a class="anchor" href="#arbitrary-initial-q-or-v">#</a>
</h2>
<p>Even give <span>
  \(Q(s,a)\)
</span>
 or <span>
  \(V(s)\)
</span>
 a positive value at start, by updating, a negative value <span>
  \(Q(s&#39;,a&#39;)\)
</span>
 or <span>
  \(V(s&#39;)\)
</span>
 will contribute part of it. At least the <span>
  \(R\)
</span>
 will definately affect negatively to it. After this, a positive <span>
  \(Q(s,a)\)
</span>
 or <span>
  \(V(s)\)
</span>
 can&rsquo;t be compared with a <span>
  \(Q(s,a)\)
</span>
, which is driven from the positive value given by terminal.</p>
<h2 id="where-goes-the-transition-function">
  Where goes the transition function?
  <a class="anchor" href="#where-goes-the-transition-function">#</a>
</h2>
<p>When we have the model, then <span>
  \(p(s&#39;,r|s,a)\)
</span>
 can help us compare the <span>
  \(V(s)\)
</span>
  by avioding the low value and passing more though the high value, or directly getting more rewards. In model free, there is no <span>
  \(p(s&#39;,r|s,a)\)
</span>
 in offer. But no matter <span>
  \(p(s&#39;,r|s,a)\)
</span>
 or <span>
  \(Q(s,a)\)
</span>
 or <span>
  \(V(s)\)
</span>
 just to find the best way. With many exploring, the value is showing the best probability of getting best reward, then there is no need to setting <span>
  \(p(s&#39;,r|s,a)\)
</span>
 in model free environment.</p>
<p><span>
  \(p(s&#39;,r|s,a)\)
</span>
 is of course not controlled by the hero.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Online image from 
  <a href="https://miro.medium.com/freeze/max/588/1*Lq_shZnfjjiFEBmBOHk_qA.gif">here</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Sutton&rsquo;s Reinforcement Book&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>The ep represents the episode number, there we use first visit Monte Calro method.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#basic-recap">Basic Recap</a>
      <ul>
        <li><a href="#a-little-more-from-basic">A little more from Basic</a></li>
        <li><a href="#the-famous-bellman-equation">The FAMOUS Bellman Equation</a></li>
        <li><a href="#choose-path-based-on-bellman-equation">Choose Path based on Bellman Equation</a></li>
        <li><a href="#v-value-vs-q-value">V value vs Q value</a></li>
      </ul>
    </li>
    <li><a href="#update-methods-clarification">Update Methods Clarification</a>
      <ul>
        <li><a href="#monte-carlo">Monte Carlo</a></li>
        <li><a href="#a-constant-monte-carlo">a-constant Monte Carlo</a></li>
        <li><a href="#temporal-difference">Temporal Difference</a></li>
        <li><a href="#q-learning">Q-learning</a></li>
      </ul>
    </li>
    <li><a href="#other-thinking">Other Thinking</a>
      <ul>
        <li><a href="#arbitrary-initial-q-or-v">Arbitrary Initial Q or V</a></li>
        <li><a href="#where-goes-the-transition-function">Where goes the transition function?</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












