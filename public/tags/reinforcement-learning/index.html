<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="reinforcement learning" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://example.com/tags/reinforcement-learning/" />

<title>reinforcement learning | 無極</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.95d69eb6bad8b9707ff2b5d8d9e31ce70a1b84f2ed7ffaf665ffcf00aa7993bd.css" integrity="sha256-ldaetrrYuXB/8rXY2eMc5wobhPLtf/r2Zf/PAKp5k70=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.4ac964f991a32eddea693121c643f3f6ee8ab06e61631b51107b4c1266e49107.js" integrity="sha256-Sslk&#43;ZGjLt3qaTEhxkPz9u6KsG5hYxtREHtMEmbkkQc=" crossorigin="anonymous"></script>

  <script defer src="/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js" integrity="sha256-b2&#43;Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC&#43;NdcPIvZhzk=" crossorigin="anonymous"></script>
<link rel="alternate" type="application/rss+xml" href="https://example.com/tags/reinforcement-learning/index.xml" title="無極" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><img src="/favicon.ico" alt="Logo" /><span>無極</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>



  



  
    
  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex justify-between">
      <a role="button" class="flex align-center">
        <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
        English
      </a>
    </label>

    <ul>
      
      <li>
        <a href="https://example.com/ru/">
          Deutsch
        </a>
      </li>
      
      <li>
        <a href="https://example.com/zh/">
          Chinese
        </a>
      </li>
      
    </ul>
  </li>
</ul>











  <ul>
<li>
<p>
  <a href="/posts/">Working&hellip;</a></p>
</li>
<li>
<p>
  <a href="/docs/">The Book</a></p>
</li>
</ul>






  
<ul>
  
  <li>
    <a href="https://github.com/alex-shpak/hugo-book" target="_blank" rel="noopener">
        Github
      </a>
  </li>
  
  <li>
    <a href="https://themes.gohugo.io/hugo-book/" target="_blank" rel="noopener">
        Hugo Themes
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>reinforcement learning</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  <nav>
  <ul>
  
    
    <li class="book-section-flat">
      <strong>Categories</strong>
      <ul>
      
        <li class="flex justify-between">
          <a href="/categories/AI/">AI</a>
          <span>7</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Book/">Book</a>
          <span>4</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/IT/">IT</a>
          <span>4</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Writing/">Writing</a>
          <span>1</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Game/">Game</a>
          <span>1</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/ThinkOfChaos/">ThinkOfChaos</a>
          <span>3</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Language/">Language</a>
          <span>1</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Diary/">Diary</a>
          <span>2</span>
        </li>
      
      </ul>
    </li>
    
  
    
    <li class="book-section-flat">
      <strong>Tags</strong>
      <ul>
      
      </ul>
    </li>
    
  
  </ul>
</nav>


  </aside>
  
 
      </header>

      
      
  
  <article class="markdown book-post">
    <h2>
      <a href="/docs/ai/rl-notes/">Reinforcement Learning Notes</a>
    </h2>
    
  <h5>December 8, 2020</h5>



  
  <div>
    
      <a href="/categories/AI/">AI</a>
  </div>
  

  
  <div>
    
      <a href="/tags/reinforcement-learning/">reinforcement learning</a>
  </div>
  




    <p>State Representation #   state vector ( obey Markov Property) observation →knowledge→ state  Partially observable Markov decision process &ldquo;learning or classification algorithms to &ldquo;learn&rdquo; those states&rdquo;  A simple linear regression A more complex non-linear function approximator, such as a multi-layer neural network.    The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results.
        <a href="/docs/ai/rl-notes/">...</a>
      
    </p>
  </article>
  
  <article class="markdown book-post">
    <h2>
      <a href="/docs/achv/ssh-jupyter/">Use Jupiter Notebook by ssh</a>
    </h2>
    
  <h5>December 5, 2020</h5>



  
  <div>
    
      <a href="/categories/IT/">IT</a>
  </div>
  

  
  <div>
    
      <a href="/tags/reinforcement-learning/">reinforcement learning</a>, 
      <a href="/tags/linux/">linux</a>, 
      <a href="/tags/jupyter/">jupyter</a>, 
      <a href="/tags/ssh/">ssh</a>
  </div>
  




    <p>This blog shows up a way to ssh a ubuntu computer (as Jupyter Notebook server) from mac.
Prerequirements #  Ubuntu / Remote Setting #  SSH Setting #  run the following command to enable the ssh on ubuntu(18.04)
$ sudo apt-get install openssh-server Jupyter Notebook Setting(optional) #  using jupyter notebook may require the token or password, you may refer to the jupyter notebook official site for details.
        <a href="/docs/achv/ssh-jupyter/">...</a>
      
    </p>
  </article>
  
  <article class="markdown book-post">
    <h2>
      <a href="/docs/ai/rl-epsilon-policy/">epsilon-Greedy vs epsilon-Soft</a>
    </h2>
    
  <h5>November 30, 2020</h5>



  
  <div>
    
      <a href="/categories/AI/">AI</a>
  </div>
  

  
  <div>
    
      <a href="/tags/reinforcement-learning/">reinforcement learning</a>
  </div>
  




    <p>In reinforcement learning, we can&rsquo;t run infinite times to update the whole $Q$ - value table or $V$ - value table, efficient update choices must be made.
Generally thinking, which $s$ or $(s,a)$ has more opportunity to get $R$ (high value), should be updated more to converge to the optimal. But stochastic exploring is also required to jump out of sub-optimal. The simple idea to implement is $\epsilon$ -greedy.
Tips:
        <a href="/docs/ai/rl-epsilon-policy/">...</a>
      
    </p>
  </article>
  
  <article class="markdown book-post">
    <h2>
      <a href="/docs/ai/rl-update-equation/">Value Update Comparsion among Basical RL</a>
    </h2>
    
  <h5>November 30, 2020</h5>



  
  <div>
    
      <a href="/categories/AI/">AI</a>
  </div>
  

  
  <div>
    
      <a href="/tags/reinforcement-learning/">reinforcement learning</a>
  </div>
  




    <p>In this post, we will compare the state value update or state-action value update equation in fundamental rl methods.
Monte Carlo #  $$V(s)\leftarrow V(s)+\frac{1}{N}[G_t-V(s)]$$
recall that, monte carlo update use average $V(s)=\sum G_t/N$. After simple transformation, we will have the error form equation:point_up_2:.
and the corresponding state-action value update is:
$$Q(s,a)\leftarrow Q(s,a)+\frac{1}{N}[G_t-Q(s,a)]$$
Temporal Difference #  TD is driven from $\alpha$ - Monte Carlo, but replace the $G_t$ with $R_t+\gamma V(s')$, since we want a instant update per step rather than till terminal.
        <a href="/docs/ai/rl-update-equation/">...</a>
      
    </p>
  </article>
  
  <article class="markdown book-post">
    <h2>
      <a href="/docs/ai/rl-comparsion/">Monte Carlo vs TD vs Q-learning</a>
    </h2>
    
  <h5>November 28, 2020</h5>



  
  <div>
    
      <a href="/categories/AI/">AI</a>
  </div>
  

  
  <div>
    
      <a href="/tags/reinforcement-learning/">reinforcement learning</a>
  </div>
  




    <p>Basic Recap #  Reinforcement learning bases on $V(s),Q(s,a),\pi(a|s),R,G$:
  $V(s)$ : state value, often used in model-based method;
  $Q(s,a)$ : state-action value, often used in model-free method;
 why state-action: $s\rightarrow a$ is defined partly in $\pi(a|s)$, and $V(s,a),\pi(a|s)$ are all parameters inside agent, consequently, $Q(s,a)$ is a combination of $V(s)$ and $\pi(a|s)$.    $\pi(a|s)$ : the policy of a agent, chose a $a$ (action) at a $s$ state;
        <a href="/docs/ai/rl-comparsion/">...</a>
      
    </p>
  </article>
  

  
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  <nav>
  <ul>
  
    
    <li class="book-section-flat">
      <strong>Categories</strong>
      <ul>
      
        <li class="flex justify-between">
          <a href="/categories/AI/">AI</a>
          <span>7</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Book/">Book</a>
          <span>4</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/IT/">IT</a>
          <span>4</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Writing/">Writing</a>
          <span>1</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Game/">Game</a>
          <span>1</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/ThinkOfChaos/">ThinkOfChaos</a>
          <span>3</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Language/">Language</a>
          <span>1</span>
        </li>
      
        <li class="flex justify-between">
          <a href="/categories/Diary/">Diary</a>
          <span>2</span>
        </li>
      
      </ul>
    </li>
    
  
    
    <li class="book-section-flat">
      <strong>Tags</strong>
      <ul>
      
      </ul>
    </li>
    
  
  </ul>
</nav>

 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












